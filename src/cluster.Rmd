---
title: "Variable clustering for Husler-Reiss graphical models"
author: "CAPEL Alexandre"
date: "`r Sys.Date()`"
output:
  html_document:
    toc_depth: 2
    number_sections: true
    fig_caption: true
    pandoc_args: ["-F","pandoc-crossref"]
  pdf_document:
    toc: true
    number_sections: true
bibliography: biblio.bib
always_allow_html: true
---


\newcommand{\indep}{\perp \!\! \perp}
\newcommand{\indic}{1\!\!1}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(graphicalExtremes,
               targets)
```

We want to use the graphical model tools for extreme value theory to build a new way of clustering variable, as done for the graphical models for Gaussian vector [@touwClusterpathGaussianGraphical2024].

Let $V = \{1, \dots, d\}$.

# Introduction


For a multivariate random variable, it can be useful to know the dependence structure between the components. Particularly, we can summarise the conditional dependence structure with a graph $\mathcal G = (V,E)$ with $E \subset V \times V$ as below : 

<center>
![Exemple of a graph](../figure/g2.png){width=60%}
</center>

$~$

For classical conditional independence (in term of density factorisation), we call such variables graphical models.

**Construction of a graph**

Let $X$ a graphical model according to the graph  $\mathcal G = (V,E)$.

Then, by definition, there is the relation : 

$$
(i,j) \in E \Longleftrightarrow X_i \indep X_j ~|~X_{V\setminus\{i,j\}}
$$
It is the pairwise Markov property.

The graphoid structure of such a relation gives us the equivalence between parwise Markov property and the global Markov property : 

$$
A \perp_\mathcal G B ~|~C \Longrightarrow  X_A \indep X_B ~|~X_C
$$
where $\perp_\mathcal G$ is the separation relation between sets of nodes.


We would like to cluster the variable using the graphical model structure to be able to get an interpretation of the clusters and then reduce the dimension of the graph. In that sense, we would have the nodes as the clusters and the edge thanks to the global Markov properties relationship existing between these. 


However for general graph, it is not easy. Indeed even with three clusters, we can have this type of situation : 

<center>
![3 differents way of clustering where $A$ is in blue, $B$ in green and $C$ in red.](../figure/clusters.png)
</center>
$~$

and each time, we have the fact that : 
$$
X_A \indep X_B ~|~X_C
$$

So we want to : 

- get a **unique** decomposition using the graphical model structure.

- link this decomposition to **a way of clustering the variables.**

# Husler-Reiss graphical model

In this section, we will present quickly the Husler-Reiss distribution in the MGPD case, and a present the notion of conditional independence in this context, together with the characterisation of this conditional independence for Husler-Reiss graphical models. 

## Definition of a Husler-Reiss model

Now we consider a MGPD random vector $Y$ indexed by $V$.

The Husler-Reiss model is a MGPD parameterized by a symmetric conditionally negative definite matrix $\Gamma$ with $\text{diag}(\Gamma) = 0$. 

One knows that every MGPD is defined by an exponent measure $\Lambda$ giving a correspondence between MGEV and MGPD (see [@rootzenMultivariateGeneralizedPareto2006]). For the case of the Husler-Reiss distribution, the exponent measure is absolutely continuous with respect to the Lebesgue measure on the cone $\mathcal E = \mathbb R_+^d \setminus \{0\}$ and its derivative is given by [@engelkeEstimationHueslerReissDistributions2012] for any $k \in V$ : 


$$
\lambda(y) = y_k^{-2} \prod_{i\neq k} y_i^{-1}\phi(\tilde y_{\setminus k}; \Sigma^{(k)}), \quad y \in \mathcal E
$$
where $\phi(.,\Sigma)$ is the density function of a gaussian vector centered with covariance matrix  $\Sigma$, $\tilde y_{\setminus k} = (\log(y_i/y_k) + \Gamma_{ik}/2)_{i \in V}$ and : 

$$
\Sigma^{(k)}_{ij} = \frac{1}{2} (\Gamma_{ik} + \Gamma_{kj}-\Gamma_{ij}) \quad \text{for } i,j \neq k
$$

which is obviously definite positive.

## Characterisation of a HRGM

For extreme value theory, the notion of conditional independence is very complicated to defined. First, for max-stable distribution with continuous positive density, the notion of conditional independence is equivalent to the global independence of the variables (Papastathopoulos reference). Moreover, for the MGPD case, the random vector is not even defined in a product space which make impossible the use of conidtional independence.

Hopefully, [@engelkeGraphicalModelsExtremes2020] build a new notion of conditional independence, adapted to MGPD distribution and then permit us to make graphical model with this type of distribution. 

Let $A, B$ and $C$ a parition of $V$. Then for MGPD random vector $Y$ indexed by $V$, we say that $Y_A$ is conditionally independent of $Y_B$ given $Y_C$ if :

$$
Y_A^k \indep Y^k_B~ |~ Y^k_C, \quad \quad \forall k  \in V.
$$ 
where $Y^k$ is defined as the vector $Y$ conditionally to the event $\{Y_k >1\}$.

We note then $Y_A \indep_e Y_B ~| ~Y_C$.

Moreover, in the same article, they give a first characterisation of the extremal conditional independence for Husler-Reiss models : 

**Proposition.** For a Husler-Reiss graphical model (HRGM) $Y$ with variogram $\Gamma$, then for all $i,j \in V$ and $k \in V$ we have : 

$$
Y_i \perp_e Y_j ~|~ Y_{\setminus \{i,j\}} \Leftrightarrow 
\begin{cases} 
\Theta^{(k)}_{ij} = 0, &\text{if } i,j \neq k, \\ 
\sum \Theta^{(k)}_{lj} = 0, &\text{if } i =k, j \neq k,\\
\sum \Theta^{(k)}_{il} = 0,&\text{if } i\neq k, j=k
\end{cases}
$$

where $\Theta^{(k)}$ is the precision matrix of $\Sigma^{(k)}$ (i.e $\Theta^{(k)} = (\Sigma^{(k)})^{-1}$). 

In [@hentschelStatisticalInferenceHuslerReiss2023], they build an extended precision matrix $\Theta$ which summarize all the information we need for the conditional independence relationship for the extremal graphical models, in that sense : 

$$
Y_i \indep_e Y_j ~|~ Y_{V\setminus\{i,j\}} \quad \Longleftrightarrow \quad \Theta_{ij} = 0.
$$

This matrix can be obtain by using some applications (which are bijections) that garanties a form of unicity of the spectral representation. 

Therefore, let's consider the following applications :
$$
\sigma: \Gamma \mapsto \Pi_d(-\frac 1 2 \Gamma) \Pi_d, \quad \quad \quad \theta: \Gamma \mapsto \sigma(\Gamma)^+
$$
where the matrix $A^+$ is the general inverse of $A$, and $\Pi_d$ the orthogonal projection matrix in the space $<\indic>^\perp$.

They show in [@hentschelStatisticalInferenceHuslerReiss2023] that the above applications are homeomorphisms between the set of the strictly conditionally negative definite variogram matrix $\mathcal D_d$ ad the set of symmetric positive semi-definite matrix with kernel equal to $<\indic>$, denoted by $\mathcal P_d^1$. More they show that : 

$$
\sigma^{-1}(\Sigma) = \gamma(\Sigma), \quad \quad \theta^{-1}(\Theta) = \gamma(\Theta^+),
$$

where $\gamma(\Sigma)=\indic \text{diag}(\Sigma)^T + \text{diag}(\Sigma) \indic^T - 2 \Sigma$.

# Clusterpath for GGM

In this section, we will present the matrix structure we will use for the Husler-Reiss graphical model. More, we will present an algorithm to estimate this graphical structure.

## Gaussian Graphical Model

In [@touwClusterpathGaussianGraphical2024], they build a graphical model that we can use for clustering, in the case of Gaussian graphical model (GGM). 

For the GGM, there exists an easy characterisation of the conditional independence which is similar to HRGM for the extreme. For a Gaussian graphical model $X$ with covariance matrix $\tilde \Sigma$, we have : 

$$
X_i \indep_e X_j ~|~ X_{V\setminus\{i,j\}} \quad \Longleftrightarrow \quad \tilde\Theta_{ij} = 0,
$$

where $\tilde \Theta = \tilde \Sigma^{-1}$, the precision matrix.

Let assume that the variable $X$ can be grouped in $K$ clusters $\{C_1, \dots, C_K\}$ with $p_k = |C_k|$.

The goal was to encouraging clustering of the graph by forcing the precision matrix to have a $K$ blocks structure as follows : 

$$
\tilde \Theta = 
\begin{pmatrix} 
(a_{1} - r_{11})I_{p_1} & 0 & \dots & 0 \\
0 & (a_{2} - r_{22})I_{p_2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots &(a_{K} - r_{KK})I_{p_K}
\end{pmatrix} + 
\begin{pmatrix} 
r_{11} \indic \indic^t & r_{12} \indic \indic^t & \dots & r_{1K} \indic \indic^t \\
r_{21} \indic \indic^t& r_{22} \indic \indic^t & \dots & r_{2K} \indic \indic^t \\
\vdots & \vdots & \ddots & \vdots \\
r_{K1} \indic \indic^t & r_{K2} \indic \indic^t & \dots &r_{KK} \indic \indic^t
\end{pmatrix},
$$
where $A$ is a $K\times K$ diagonal matrix, $R$ a $K\times K$ symmetric matrix, $I_p$ the $p \times p$identity matrix.

We can then get this type of "graph factorisation" which is unique due to precision matrix structure : 



<center>
![Graph factorisation thanks to the precision matrix structure](../figure/cluster_graph.png){width=90%}
</center>

$~$

Thus, with this factorisation, we build three clusters with an interpretation of conditional independence between them. 

For the estimation of the precision matrix, they use the following likelihood : 

$$
L(\Theta) = - \log(|\Theta|) + \text{tr}(\Sigma\Theta),
$$

where $\log(|\cdot|)$ is the logarithm of the determinant and $\text{tr}(\cdot)$ the trace.

To get the maximum likelihood, they use a convex penalty to get the unknown block structure of the precision matrix. Thus, we got this optimisation program : 

$$
\hat \Theta = \arg \min_\Theta \Big[- \log(|\Theta|) + \text{tr}(\overline \Sigma\Theta) + \lambda\mathcal P(\Theta)\Big], \quad \quad s.t. \Theta^t = \Theta, ~~\Theta >0
$$

where $\overline \Sigma$ is an estimation of the covariance matrix $\tilde \Sigma$.


From now, as we have for $i,j \in C_k$ that $\theta_{\cdot i} = \theta_{\cdot j}$, we will note $\theta_{C_k}$ the vector of the precision matrix of the cluster $C_k$.



## Clusterpath algorithm

In order to find the groups in precision matrix $\tilde \Theta$, we will use the clusterpath algorithm from [@hockingClusterpathAlgorithmClustering]. 

For these convex optimisation programs, we impose to the penalty function to be of the form : 
$$
\mathcal P(\Theta) = \sum_{i<j} w_{ij} D(\theta_{\cdot i}, \theta_{\cdot j}),
$$
where $w_{ij}$ are some positive weights, and $D$ a distance in $\mathbb R^d$.

**The distance $D$**

We can use a lot of distance : 

- with the $l^p$ norm for $p\in [1, \infty]$.

- in particular $l^1$, $l^2$ and $l^\infty$.

- in [@touwClusterpathGaussianGraphical2024] they use another distance defined as : 
$$
D(\theta_{.i}, \theta_{.j}) = \sqrt{(\theta_{ii} - \theta_{jj})^2 + \sum_{k\neq i,j} (\theta_{ik} - \theta_{jk})^2}
$$
which "can be interpreted as a group lasso penalty". 


**Choice of $w_{ij}$**

The choice of $w_{ij}$ is also free, even if they present one which seems better (or nearer from the data) using : 
$$
w_{ij} = \exp(-\chi ||\theta_{\cdot i} - \theta_{\cdot j}||^2)
$$

where $||.||$ is the $l^2$ norm.

**Clusterpath algorithm** 

The algorithm is a gradient descent algorithm, adding conditions to detect clusters and fuse variables. 

\newpage

| Algorithm 1 : Clusterpath   |
| :--------------- |
|**Input:** initial guess $\Theta$, initial estimation $\overline\Sigma$, initial clusters, weight $w_{ij}$, regularisation $\lambda$ |
|G <- `gradient(.)`  |
|**while** $||G|| > \varepsilon$ **do** |
| $\quad$ $\Theta$ <- `step_grad(.)`
| $\quad$ $\Theta$, clusters <- `detect_cluster(.)`
| $\quad$ G <- `gradient(.)`  |
|**end while** |
|**return** $\Theta$, clusters|

The `gradient` function depends on all the parameters, `step_grad(.)` is just the step part of a gradient descent algorithm : we update the estimation by :

$$
\hat\Theta_{k+1} \leftarrow \hat\Theta_k - h \times \nabla L(\Theta)
$$

For the `detect_cluster(.)`, the clusters merged if the distance between two groups $C_1$ and $C_2$ is under a small threshold. Then, the coefficient of the new cluster $C$ is computed by the weighted mean of the two other one : 

$$
\theta_C = \frac{|C_1| \theta_{C_1}+|C_2| \theta_{C_2}}{|C_1| +|C_2|}.
$$

We can also try to fuse clusters if the cost function decreases if merging.

# Clusterpath adaptated for HRGM

Now we want to adapt the previous method to the Husler-Reiss graphical models.

## Maximum likelihood for graphical model

For the estimation of the precision matrix for HRGM, [@hentschelStatisticalInferenceHuslerReiss2023] shows that find the $\Theta$ is equivalent to minimise :  
$$
L(\Theta) =  -\log(|\Theta|_+) - \frac12 tr(\overline \Gamma\Theta),
$$

where $\overline \Gamma$ is an estimation of the variogram matrix $\Gamma$ and $|\cdot|_+$ the generalised determinant. 

For the next, we will first use the $l^2$ norm penalty and we will try to minimize : 

$$
L_\mathcal P(\Theta, \lambda) = L(\Theta) + \lambda \mathcal P(\Theta)
$$

with $\lambda >0$ and $\Theta \in \mathcal P_d^1$.

## Adaptation of the expression

As $\Theta \in \mathcal P_d^1$, there are supplementary conditions on the matrix : the rows must sum to one !

It follows that : 
$$
a_{k} = r_{kk} - \sum_{j=1}^K p_j r_{kj}, \quad \quad \forall k \in\{1, \dots, K\}
$$

More, we can rewrite the matrix $\Theta$ as follow [@touwClusterpathGaussianGraphical2024] :

$$
\Theta = 
\begin{pmatrix} 
(a_{1} - r_{11})I_{p_1} & 0 & \dots & 0 \\
0 & (a_{2} - r_{22})I_{p_2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots &(a_{K} - r_{KK})I_{p_K}
\end{pmatrix} + 
U R U^t,
$$

where $U$ is a $d\times K$ matrix such that $u_{ij} = 1$ if $i\in C_j$ and $0$ otherwise. 

Then we can deduce the computation of the trace of $\overline \Gamma \Theta$ : 

$$
tr(\overline \Gamma \Theta)= \text{tr}(\overline \Gamma URU^t) + \sum_{k=1}^K(a_{k} - r_{kk})\text{tr}(\overline \Gamma_{C_k})
$$

with $\overline \Gamma_{C_k}$ the $p_k \times p_k$ matrix computed from $\overline \Gamma$ with the indices in $C_k$. But as $\overline \Gamma$ get only zero in its diagonal, we finally obtain : 

$$
tr(\overline \Gamma \Theta) = \text{tr}(\overline \Gamma URU^t)
$$

**Adaptation of the distance**

Let's take square $l^2$ norm penalty for $\mathcal P$. 

For $i, j$ in the same cluster $C_k$ we have : 

$$
||\theta_{\cdot i} - \theta_{\cdot j}||^2 = 2(a_{k} - r_{kk})^2
$$

Here, we wish that for two variables in the same cluster, this distance is equal to zero. 

The distance $D$ from [@touwClusterpathGaussianGraphical2024] is built for this reason. But in our case we can upgrade the distance : indeed, our matrix $\Theta$ have an additionally constraint that rows sum to $0$. Thus, we can remove the $(\theta_{ii} - \theta_{jj})^2$ term as the latter is obviously equal to zero if we are in the same cluster. 

Therefore, we will consider the following squared distance for the next : 

$$
D^2(\theta_{.i}, \theta_{.j}) =  \sum_{t\neq i,j} (\theta_{it} - \theta_{jt})^2
$$


Now, it is time to write the penalty formula with the $R$ matrix. We have :

- for $i,j$ in the same cluster $D^2(\theta_{.i}, \theta_{.j}) =0$ (it is the goal of this distance).

- for $i, j$ in respectively the clusters $C_k$ and $C_l$ we have : 

\begin{align*}
D^2(\theta_{.i}, \theta_{.j}) =& \sum_{t\neq i,j} (\theta_{it} - \theta_{jt})^2 \\ 
 = &\sum_{q \neq k,l} p_m (r_{kq}-r_{lq})^2 +(p_k-1) (r_{kk}-r_{lk})^2+(p_l-1) (r_{ll}-r_{lk})^2\\ 
 = & \tilde D^2(r_{\cdot k}, r_{\cdot l}).
\end{align*}


Then, by grouping all the terms in $\mathcal P$, we get : 

$$
\mathcal P(R) = \sum_{l>k} W_{kl} \tilde D^2(r_{\cdot k}, r_{\cdot l})
$$

with $W_{lk} = \sum_{i \in C_k}\sum_{j \in C_l}w_{ij}$.

Our likelihood can now be expressed as a function of the $R$-matrix. 


## Computation of the derivative

We can see that the penalized negative log-likelihood can be decomposed as a sum of three element : 

$$
L_\mathcal P(R, \lambda) = L_{log}(R) + L_{trace}(R) + \lambda L_{pen}(R)
$$

So we just need to compute separately all the derivative for each coefficient (in the upper triangular part, by symmetry). 

### Gradient of $L_{log}(R)$

Let's denote $E = \{(i,j) \in [\!|1, d |\! ]^2, i < j\}$ and $F = \{(k,l) \in [\!|1, K |\! ]^2, k\le l \}$ and we consider $\mathbb R^E$ and $\mathbb R^F$ the set of real-valued vector indexed by $E$ anf $F$.

One can show that there exists a function $f : \mathbb R^F \rightarrow \mathbb R^E$ such that :

$$
f(R) = U(\Theta) 
$$

where $U$ is the application defined in [@hentschelStatisticalInferenceHuslerReiss2023] which maps a matrix to a vector in $\mathbb R^E$ containing the entries in the upper triangular part of the matrix.

Then we can defined also the function $g :\mathbb R^E \rightarrow \mathbb R$ as : 
$$
g(U(\Theta)) = - \log(|\Theta|_+)
$$
which is well defined by symmetry of $\Theta$.

Thus we can see our sub-loglikelihood as : 
$$
L_{log}(R) = g \circ f(R)
$$

And then use the chain rule formula to get the partial derivative for $n \in F$ : 

$$
\frac{\partial L_{log}(R)}{\partial r_n} = \sum_{m \in E} \frac{\partial g}{\partial \theta_{m}}(f(R)) \times \frac{\partial f_m}{\partial r_n}(R)
$$

In the appendix of [@hentschelStatisticalInferenceHuslerReiss2023], they show that : 

$$
\frac{\partial g(U(\Theta))}{\partial \theta_m} =  \gamma(\Theta^+)_m, \quad \quad \text{for } m \in E
$$

Now we just have to compute the derivative of $f$. Let $m = (i,j) \in E$.
We can stress first that : 

$$
f_m(R) =r_{kl},  \quad \text{if } i\in C_k, ~ j \in C_l,~ \text{or } i\in C_l, ~ j \in C_k,~
$$

Then we can deduce the expression below : 

$$
\frac{\partial f}{\partial r_{kl}} =
\begin{cases} 
1  &\text{if } i \in C_k, ~j \in C_l,\text{ or } i \in C_l,~ j \in C_k, \\ 
0 & \text{otherwise} \end{cases}
$$

We can now associate these expressions to deduce for $k < l$ that : 

\begin{align*}
\frac{\partial L_{log}(R)}{\partial r_{kl}} &= \sum_{i<j} \gamma(\Theta^+)_{ij}\Big[ \indic_{i\in C_k}\indic_{j\in C_l}+\indic_{i\in C_l}\indic_{j\in C_k}\Big] \\
& = \sum_{i \in C_k}\sum_{j \in C_l}\gamma(\Theta^+)_{ij} \\
& = u_k^t\gamma(\Theta^+)u_l ,
\end{align*}

with $u_k$ the k-th column of the matrix of clusters $U$ and because $\gamma(\Theta^+)_{ii} = 0$. 

Moreover if $k=l$, we get : 

\begin{align*}
\frac{\partial L_{log}(R)}{\partial r_{kk}} &= \sum_{\underset{i<j}{i,j \in C_k}}\gamma(\Theta^+)_{ij} \\
&=  \frac 12 u_k^t\gamma(\Theta^+)u_k
\end{align*}

that ends the calculation.

**Warning.** When $p_k = 1$, we have $\frac{\partial L_{log}(R)}{\partial r_{kk}} =0$.

### Gradient of $L_{trace}(R)$

We recall that : 

$$
L_{trace}(R) = -\frac 1 2\text{tr}(\overline \Gamma URU^t)
$$

As we have for all symmetric matrix $A$ : 
$$
\frac{\partial tr(AB)}{\partial b_{ij}} = \begin{cases} 2 a_{ij} & \text{if } i \neq j, \\ a_{ii} & \text{otherwise.} \end{cases}
$$

Then we can deduce that for $(k,l) \in F$ : 
$$
\frac{\partial L_{trace}(R)}{\partial r_{kl}} =\begin{cases} -(U^t\overline \Gamma U)_{kl} & \text{if } k=l, \\  -\frac 1 2(U^t\overline \Gamma U)_{kk} & \text{otherwise.}\end{cases}
$$

**Warning.** Like the previous section, when $p_k = 1$, we have $\frac{\partial L_{trace}(R)}{\partial r_{kk}} =0$.

### Gradient of $L_{pen}(R)$


We recall that : 

$$
L_{pen}(R) = \sum_{l'>k'} W_{k'l'} \tilde D^2(r_{\cdot k'}, r_{\cdot l'})
$$

So we just need to compute the derivative of $\tilde D^2$ for each coefficient $r_{kl}$. 

Let $(k,l) \in F$.

Thus, for $k<l$, we have : 

$$
\frac{\partial D^2(r_{\cdot k'}, r_{\cdot l'})}{\partial r_{kl}} =
\begin{cases} 
2p_k(r_{kl} -r_{kk'}) & \text{if } k \neq k', l = l'\\ 
2p_l(r_{kl} -r_{ll'}) & \text{if } k = k', l \neq l'\\ 
2(p_k-1)(r_{kl} -r_{kk}) +2(p_l-1)(r_{kl} -r_{ll}) & \text{if } k = k', l = l'\\ 
0 & \text{otherwise.}
\end{cases}
$$

and for $k'<l'$ : 

$$
\frac{\partial D^2(r_{\cdot k'}, r_{\cdot l'})}{\partial r_{kk}} =
\begin{cases} 
2(p_k-1)(r_{kk} -r_{kl'}) & \text{if } k = k' \\ 
2(p_k-1)(r_{kk} -r_{k'k}) & \text{if } k = l'\\ 
0 & \text{otherwise.}
\end{cases}
$$

We can deduce the derivatives : 

$$
\nabla L_{pen}(R) = \sum_{k'<l'} W_{k'l'}\nabla \tilde D^2_{k'l'}(R)
$$

where $\tilde D^2_{k'l'}(R) = D^2(r_{\cdot k'}, r_{\cdot l'})$.


## Choice of a step for descent

In the gradient descent algorithm, we compute : 

$$
R^{(t+1)} \leftarrow R^{(t)} - h \nabla_R L(R), \quad \text{with } h>0
$$ 

where $h$ is called the step size.

However, in that case, we need to verify that after the step, $f(R^{(t+1)})$ is still a symmetric positive matrix. 

### Research of relationship between $R$ and $f(R) = \Theta$

In this section, we want to find condition on $R$ to make the $Theta$ matrix positive. 

First, let's define the $K \times K$ alternative matrix of clusters :
$$
\tilde R = RP - T
$$ 
where $P = \text{diag}((p_k)_{k =1,\dots, K})$ and $T = \text{diag}((Rp)_{k=1, \dots, K})$.

**Proposition.** Let $R$ and $\Theta = f(R)$. Assume that : 

- $\forall k \in \{1,\dots, K\}, a_k - r_{kk} \ge 0$.

- the alternative matrix of clusters $\tilde R$ is positive. 

Then $\Theta$ is positive. 

**Proof.** A characterization of the positiveness of a matrix is that all its eigen values are positive. We want to get the expression of the latters with the coefficient of $R$, so we need to find the roots of the characteristic polynomial.

\begin{align*}
P_\Theta(X) = &\det(\Theta - X I_d) \\
= & \begin{vmatrix}
A_1 - XI_{p_1} & r_{12} \indic \indic^t& \cdots & \cdots & r_{1K} \indic \indic^t\\
r_{21} \indic \indic^t&A_2 - XI_{p_2} & \ddots&  &  \vdots \\
\vdots &  \ddots & \ddots& \ddots &\vdots \\
\vdots & & \ddots & A_{K-1}- XI_{p_{K-1}}&  r_{KK-1} \indic \indic^t \\
r_{K1} \indic \indic^t & \cdots & \cdots &r_{KK-1} \indic \indic^t  & A_K - XI_{p_K}
\end{vmatrix}
\end{align*}

with $A_k = \begin{pmatrix} a_k & r_{kk} & \cdots& r_{kk} \\ r_{kk} & a_k & \ddots& \vdots \\ \vdots & \ddots& \ddots & r_{kk} \\ r_{kk} & \cdots & r_{kk} & a_{kk}\end{pmatrix}$.

First, we can focus our calculation on the $p_1$ first lines. If we subtract the $p_1 - 1$ first lines by the $p_1-th$, we obtain : 

 
\begin{align*}
&P_\Theta(X)  =  \\ 
 & \begin{vmatrix}
a_1- r_{11}-X \quad \quad\quad 0  \quad\quad \quad\cdots  \quad\quad\quad  0 \quad \quad \quad \quad -(a_1- r_{11}-X) & 0 & & \cdots & 0\\
~~~0 \quad\quad\quad\quad\quad \ddots \quad\quad ~\ddots \quad~~~\quad \vdots \quad\quad\quad\quad\quad\quad\quad\quad \vdots  \quad & \vdots & & &  \vdots \\
\quad\quad\quad\vdots ~~~~~~~ \quad\quad\ddots \quad\quad\quad\ddots\quad\quad\quad  ~ 0 \quad\quad \quad \quad \quad \quad \quad \quad  \vdots \quad \quad\quad & \vdots & & & \vdots \\ 
\quad\quad\quad 0 \quad\quad\quad\quad\cdots\quad\quad \quad 0  \quad \quad  a_1 - r_{11} -X  ~~   \quad -(a_1- r_{11}-X) & 0 & & \cdots & 0 \\
r_{11} \quad\quad\quad \quad ~ \quad\quad\cdots ~\quad\quad \quad\quad\quad\quad r_{11} ~~ \quad\quad a_1 - X & r_{12} \indic ^t & & \cdots & r_{1K} \indic^t \\
r_{21} \indic \indic^t&A_2 - XI_{p_2} & \ddots&  &  \vdots \\
\vdots &  \ddots & \ddots& \ddots &\vdots \\
\vdots & & \ddots & A_{K-1}- XI_{p_{K-1}}&  r_{KK-1} \indic \indic^t \\
r_{K1} \indic \indic^t & \cdots & \cdots &r_{KK-1} \indic \indic^t  & A_K - XI_{p_K}
\end{vmatrix} \\ 
= & (a_1 - r_{11} - X)^{p_1-1}\begin{vmatrix}
~1 ~~\quad 0  \quad ~~  \cdots  ~~~ \quad 0 \quad -1 ~~~& 0 & & \cdots & 0\\
~0  ~ ~~~\ddots \quad   \ddots \quad~~\vdots \quad \quad\vdots  \quad  & \vdots & & &  \vdots \\
\vdots\quad \ddots \quad ~\ddots \quad  0  \quad \quad   \vdots ~~ & \vdots & & & \vdots \\ 
0 \quad \cdots \quad 0 \quad\quad~~ 1 \quad -1 & 0 & & \cdots & 0 \\
\quad r_{11}  \quad  \quad\cdots  \quad \quad r_{11} \quad  a_1 - X & r_{12} \indic ^t & & \cdots & r_{1K} \indic^t \\
r_{21} \indic \indic^t&A_2 - XI_{p_2} & \ddots&  &  \vdots \\
\vdots &  \ddots & \ddots& \ddots &\vdots \\
\vdots & & \ddots & A_{K-1}- XI_{p_{K-1}}&  r_{KK-1} \indic \indic^t \\
r_{K1} \indic \indic^t & \cdots & \cdots &r_{KK-1} \indic \indic^t  & A_K - XI_{p_K}
\end{vmatrix}
\end{align*}

Thus we can reproduce these operations in each $p_k$ blocks. Finally, by summing the one column with the minus one, we get :

$$
P_\Theta(X) =\prod_{k = 1} ^K (a_k - r_{kk} - X)^{p_k-1} \det(\tilde R)
$$

Obviously, if the condition of the proposition are true, then the root of this polynomial are all positive. 

<div style="text-align:right;">$\square$</div>


### Gridline search for the optimal step size

With the previous proposition, we can have an upper bound for the step size $h$. Indeed, let's note $\delta_{kl}$ the gradient along $r_{kl}$. Then, if the previous step $R^{(t)}$  verifies the conditions of the proposition, the optimal step size $h^*$ should verify : 

\begin{align*}
&\forall k\in \{1,\dots, K\},~ a^{(t+1)}_k - r^{(t+1)}_{kk} \ge 0  \\ 
\Longleftrightarrow ~&\forall k\in \{1,\dots, K\}, ~\sum_{l=1}^K p_lr^{(t+1)}_{lk} \le 0 \\
\Longleftrightarrow ~&\forall k\in \{1,\dots, K\},~ \sum_{l=1}^K p_l(r^{(t)}_{lk} - h^*\delta_{lk}) \le 0 \\
\Longleftrightarrow ~&\forall k\in \{1,\dots, K\},~ \sum_{l=1}^K p_lr^{(t)}_{lk}  \le h^* \sum_{l=1}^K p_l \delta_{lk}
\end{align*}

Therefore, we obtain the cases : 

- if $\sum_{l=1}^K p_l \delta_{lk} > 0$ then $h^* \ge \sum_{l=1}^K p_lr^{(t)}_{lk} /\sum_{l=1}^K p_l \delta_{lk}$ which is a negative number as $R^{(t)}$ follows the relations. So, as $h^* > 0$, there is no constraint in this case.

- if $\sum_{l=1}^K p_l \delta_{lk} < 0$ then $h^* \le \sum_{l=1}^K p_lr^{(t)}_{lk} /\sum_{l=1}^K p_l \delta_{lk}$.

- if $\sum_{l=1}^K p_l \delta_{lk} = 0$, no constraint as well except non negative.

Thus, we will set our grid line in the segment $(0, h_{max}]$ where :

$$
h_{max} = \min_{k=1, \dots , K}\Big |\frac{\sum_{l=1}^K p_lr^{(t)}_{lk}}{\sum_{l=1}^K p_l \delta_{lk}}\Big |
$$

and we will choose the value $h^*$ which minimizes the objective function provided that the alternative matrix of clusters $\tilde R^{(t+1)}$ is positive. 


## Simulation study

Now, we want to check if the algorithm is working on basic simulation of Husler-Reiss data. For the simulation, we will use the function `rmpareto()` from the `R`-package `graphicalExtremes` developed in [@engelkeGraphicalModelsExtremes2020].

### First simulation

We will first try our algorithm on easy example. We will assume that the Husler-Reiss model gets only two clusters of size $4$ and $3$. We will fix the following values :

$$
R = \begin{pmatrix} 0.5 &-2\\ -2 & 1 \end{pmatrix}
$$

and the clusters are $C_1 = \{1,2,3,4\}$ and $C_2 = \{5,6,7\}$.

*We can also verify that this matrix follows the condition of the previous section.*
\newpage 
<center>
![Graph factorisation for the first simulation.](../figure/first_sim.png)
</center>

$~$

We can then deduce the precision matrix $\Theta$ of our model : 

$$
\Theta = \begin{pmatrix} 
4.5 & 0.5 & 0.5 &0.5& -2 & -2 & -2\\ 
0.5 & 4.5 & 0.5 &0.5& -2 & -2 & -2 \\
0.5 & 0.5 & 4.5 &0.5& -2 & -2 & -2 \\
0.5 & 0.5 & 0.5 &4.5& -2 & -2 & -2 \\
-2 & -2 & -2 & -2 & 6 & 1 & 1 \\
-2 & -2 & -2 & -2 & 1 & 6 & 1 \\
-2 & -2 & -2 & -2 & 1 & 1 & 6 \\
\end{pmatrix}
$$

and thus the variogram of our Husler-Reiss graphical model (using `Theta2Gamma()`  function) : 
```{r, first.sim.param, echo = FALSE}
tar_load(first_sim_param_cluster)

Gamma <- first_sim_param_cluster$Gamma
```

\begin{equation*}
 \Gamma = 
 \begin{pmatrix}
  `r Gamma[1,1]` & `r Gamma[1,2]` & `r Gamma[1,3]`& `r Gamma[1,4]` & `r Gamma[1,5]` & `r Gamma[1,6]` & `r Gamma[1,7]` \\
  `r Gamma[2,1]` & `r Gamma[2,2]` & `r Gamma[2,3]`& `r Gamma[2,4]` & `r Gamma[2,5]` & `r Gamma[2,6]` & `r Gamma[2,7]` \\
  `r Gamma[3,1]` & `r Gamma[3,2]` & `r Gamma[3,3]`& `r Gamma[3,4]` & `r Gamma[3,5]` & `r Gamma[3,6]` & `r Gamma[3,7]` \\
  `r Gamma[4,1]` & `r Gamma[4,2]` & `r Gamma[4,3]`& `r Gamma[4,4]` & `r Gamma[4,5]` & `r Gamma[4,6]` & `r Gamma[4,7]` \\
  `r Gamma[5,1]` & `r Gamma[5,2]` & `r Gamma[5,3]`& `r Gamma[5,4]` & `r Gamma[5,5]` & `r Gamma[5,6]` & `r Gamma[5,7]` \\
  `r Gamma[6,1]` & `r Gamma[6,2]` & `r Gamma[6,3]`& `r Gamma[6,4]` & `r Gamma[6,5]` & `r Gamma[6,6]` & `r Gamma[6,7]` \\
  `r Gamma[7,1]` & `r Gamma[7,2]` & `r Gamma[7,3]`& `r Gamma[7,4]` & `r Gamma[7,5]` & `r Gamma[7,6]` & `r Gamma[7,7]` \\
 \end{pmatrix}
\end{equation*}

Now, we have the parameter to build simulations and try to cluster the variable according to the underlying structure. 

**Setup** 


- we will simulate $n = 2000$ variables following Husler-Reiss multivariate Pareto distribution, with the upper variogram. 

- we will estimate the variogram using the empirical extremal variogram estimator (using the function `emp_vario()`). The empirical precision matrix $\overline \Theta$ is deduced from this previous estimation.

- we will use the exponential weights defined by : 
$$
w_{ij} = \exp(- \chi D^2(\overline \theta_{i, \cdot}, \overline\theta_{j, \cdot}))
$$
where the tune parameter $\chi$ is equal to one.

- the merging threshold in `merge_clusters()` is set at $\varepsilon = 10^{-1}$.

Moreover, the optimisation will begin with no assumption in the clusters, and so there is as much clusters as variables.

Finally, we will choose the $\lambda$ parameter using a grid and taking the one which produce the smallest penalised negative log-likelihood. 

```{r load-results-optim, echo = FALSE}
tar_load(first_sim_optimisation_results)
tar_load(first_sim_optimisation_no_penalty)
```

For the results, we get : 

- an optimal parameter $\lambda$ equal to `r first_sim_optimisation_results$lambda_optim`.

- we obtained the right clusters $C_1$ and $C_2$. 

- we estimated the $\hat R$ matrix as : 

```{r R-result, echo = FALSE}
R_res <- first_sim_optimisation_results$results$R
```


\begin{equation*}
\hat R = 
\begin{pmatrix} 
`r R_res[1,1]` & `r R_res[1,2]` \\
`r R_res[2,1]` & `r R_res[2,2]`
\end{pmatrix}
\end{equation*}

- a negative log-likelihood which takes the value `r round(first_sim_optimisation_results$results$nllh, 2)` whereas it takes the value `r round(first_sim_optimisation_no_penalty$nllh, 2)` for non-penalised optimization. 



# Application on fligth delay data 









### References





